_I am in the process of cleaning up this codebase. If you cannot wait for that, everything should work, but it's a bit of a mess. `experiments/gpt-mup.py` is the entry point for the main experiment. If you get stuck feel free to reach out._

# Universal pre-training

Codebase for the paper "Universal pre-training by iterated random computation".

## Installation instructions

First install the [former]() package. Download or clone it into a separate dir and install with `pip install -e .` from the root directory. **Dot not copy it to the root directory of the up repository**.

Then, download or clone the `up` repository (again, into a separate directory). Install with `pip install -e .` from the root directory.
